# Human Feedback  Pseudo-Code

## Initialization
- Init Policy $\pi$
- Init reward function $\hat{r}$
- Create empty dataset for segment & prefs dataset
- initialize gymnasium environment

## While True:
### Process 1:
- Policy interacts with environment, generates episodes (also called trajectories).
- replace real rewards from environment with those generated by  $\hat{r}$.


### Process 2:
- Grab two segments $(\sigma_1, \sigma_2)$ from the trajectories. Send them to synthetic oracle.
- We will do it like this: Grab two episodes, get the length of the shortest one. Then, get the first that many steps from the longer episode.
- We plug $(\sigma_1, \sigma_2, \mu)$ into a database


### Process 3:
- optimize $\hat{r}$ via cross entropy loss supervised learning on the collected dataset so far.